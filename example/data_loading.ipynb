{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "We first load and preprocess load demand data from [Albania]() from a Parquet file format. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet(\"../data/albania_res.parquet\")\n",
    "data.index = pd.to_datetime(data.index, utc=\"UTC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we  sett up data transformation and scaling for time series forecasting. The goal is to prepare the dataset for modeling by defining the necessary parameters for the transformation and scaling processes.\n",
    "\n",
    "For this purpose we use ``DatasetObjective`` A utility from mlpforecast for managing dataset objectives, crucial for time series forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlpforecast.data.transform import DatasetObjective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To achive this we define set of variables as follow\n",
    "\n",
    "- target_series: The target variable to be predicted; in this case, \"NetLoad\".\n",
    "- unknown_features: Any fnumerical eatures not observed in the future etc demand price.\n",
    "- calendar_variables: Features derived from the calendar, such as hour and session.\n",
    "- known_calendar_features: Precomputed features that represent the calendar variables.\n",
    "- known_continuous_features: Continuous features used in the modeling process, including lagged variables and temperature.\n",
    "- input_window_size: The size of the input window for the model (96 time steps).\n",
    "- forecast_horizon: The forecasting horizon (48 time steps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_params = {\n",
    "    \"target_series\": [\"NetLoad\"],\n",
    "    \"unknown_features\": [],\n",
    "    \"calendar_variables\": [\"HOUR\", \"Session\"],\n",
    "    \"known_calendar_features\": [\"HOUR-cosin\", \"Session-cosin\"],\n",
    "    \"known_continuous_features\": [\"NetLoad_lag_48\", \"NetLoad_lag_336\", \"Temperature\"],\n",
    "    \"input_window_size\": 96,\n",
    "    \"forecast_horizon\": 48,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we specify how we want to transform the data adding more features such as lagging or rolling window and scaling function using sklaern processing scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import (\n",
    "    MinMaxScaler,\n",
    "    RobustScaler,\n",
    "    SplineTransformer,\n",
    "    PowerTransformer,\n",
    ")\n",
    "\n",
    "data_params = {\n",
    "    \"input_scaler\": PowerTransformer(),\n",
    "    \"target_scaler\": PowerTransformer(),\n",
    "    \"lags\": [1, 7],\n",
    "    \"windows\": [],\n",
    "    \"window_funcs\": [\"mean\"],\n",
    "    \"period\": \"30min\",\n",
    "    \"date_column\": \"timestamp\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We update the parameters and define the  ``DatasetObjective``. This line ```data_params.update(common_params)`` merges common_params into data_params, ensuring all relevant settings are included for the dataset transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_params.update(common_params)\n",
    "ds = DatasetObjective(**data_params)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Since the dataset objective is an instance of sklearn object it implement two main function ```fit``` and ```transform```. The ```fit```  fit the DatasetObjective object (ds) to the provided data, ensuring that the transformations and scaling parameters are applied correctly to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.fit(data.reset_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This fitted data processing pipeline associated with the DatasetObjective instance contains all the transformation steps that will be applied to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.data_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply the fitted transform to the data we call ```transform``` function. The transformed data will produce sequences of features and targets that can be directly consumed by a model, streamlining the forecasting process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = ds.transform(data.reset_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fitting and transforming the dataset, you can proceed to analyze the transformed data, and train forecasting models using the prepared feature and target sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "az.style.use([\"science\", \"arviz-doc\", \"tableau-colorblind10\"])\n",
    "nice_fonts = {\n",
    "    \"savefig.bbox\": \"tight\",\n",
    "    \"savefig.pad_inches\": 0.05,\n",
    "    \"axes.labelsize\": 8,\n",
    "    \"font.size\": 8,\n",
    "    \"legend.fontsize\": 8,\n",
    "    \"legend.frameon\": False,\n",
    "}\n",
    "matplotlib.rcParams.update(nice_fonts)\n",
    "import matplotlib_inline.backend_inline\n",
    "\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats(\n",
    "    \"svg\", \"pdf\", \"retina\"\n",
    ")  # For export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Visualize the transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(4, 2))\n",
    "ax.plot(x[0, :, :4]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Forecasting Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build the forecasting model, we first create an instance of ```MLPForecast```, a forecasting model from mlpforecast designed to handle time series forecasting tasks. One of the inputs to ```MLPForecast``` is the data pipeline created in the preceding cell. The  we define the following `model_hparams` as a dictionary.\n",
    "\n",
    "- **data_pipeline**: The fitted data pipeline from the previous steps.\n",
    "- **embedding_size**: Size of the embedding layer.\n",
    "- **embedding_type**: Type of embedding to use (set to `None` here).\n",
    "- **combination_type**: Method for combining features (set to `\"addition-comb\"`).\n",
    "- **hidden_size**: Number of hidden units in the model (256).\n",
    "- **num_layers**: Number of layers in the model (2).\n",
    "- **expansion_factor**: Factor for expanding the dimensionality of the data (2).\n",
    "- **residual**: Whether to use residual connections (set to `True`).\n",
    "- **activation_function**: Activation function for the model (set to `\"SiLU\"`).\n",
    "- **out_activation_function**: Activation function for output (set to `\"Identity\"`).\n",
    "- **dropout_rate**: Rate of dropout for regularization (0.2).\n",
    "- **alpha**: Parameter for the model (0.5).\n",
    "- **max_epochs**: Maximum number of epochs for training (10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlpforecast.forecaster.mlp import MLPForecast\n",
    "\n",
    "model_hparams = {\n",
    "    \"data_pipeline\": ds,\n",
    "    \"embedding_size\": 20,\n",
    "    \"embedding_type\": None,\n",
    "    \"combination_type\": \"addition-comb\",\n",
    "    \"hidden_size\": 256,\n",
    "    \"num_layers\": 2,\n",
    "    \"expansion_factor\": 2,\n",
    "    \"residual\": True,\n",
    "    \"activation_function\": \"SiLU\",\n",
    "    \"out_activation_function\": \"Identity\",\n",
    "    \"dropout_rate\": 0.2,\n",
    "    \"alpha\": 0.5,\n",
    "    \"max_epochs\": 10,\n",
    "}\n",
    "model_hparams.update(common_params)\n",
    "model = MLPForecast(\n",
    "    exp_name=\"test\", model_type=\"MLPF\", hparams=model_hparams, rich_progress_bar=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "To train the model, use the following code:\n",
    "\n",
    "```python\n",
    "model.fit()\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(data.reset_index()[:20000])\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make predictions, use the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.predict(data.reset_index()[20000 : 20000 + 48 * 7 * 2])\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command predicts the next 48*7*2 time steps based on the dataset. we can visualize the predictions as follows using ```mlpforecast.plot``` functionality\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlpforecast.plot.visual_functions import plot_prediction\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(9, 2))\n",
    "ax = plot_prediction(ax, true=out[\"NetLoad\"].values, mu=out[\"NetLoad_forecast\"].values)\n",
    "ax.set_ylim(0, 1500)\n",
    "ax.set_ylabel(\"Power (MWh)\")\n",
    "fig.tight_layout(pad=1.08, h_pad=0.5, w_pad=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code create a plot showing the true values versus the predicted values for \"NetLoad\", with the y-axis labeled in MWh and proper layout settings applied.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.metrics.groupby(\"target\")[[\"MSE\", \"SMAPE\"]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backtesting the Model\n",
    "The library allows you to perform backtesting to evaluate the model across multiple time splits. Use the following code to set up backtesting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlpforecast.evaluation.backtester import BacktestingForecast\n",
    "\n",
    "backtest = BacktestingForecast(\n",
    "    forecast_len=4,\n",
    "    incremental_len=2,\n",
    "    min_train_len=12,\n",
    "    n_splits=10,\n",
    "    window_type=\"expanding\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you need to set up the model using partial:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "model = partial(MLPForecast, hparams=model_hparams, exp_name=\"bactesting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform the backtesting, run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df, metric_df = backtest.fit(data.reset_index(), model_instance=model)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to evaluate the results, you can compute the mean of metrics across folds:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df.groupby([\"target\", \"Folds\"])[\"MSE\", \"SMAPE\"].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLPF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
