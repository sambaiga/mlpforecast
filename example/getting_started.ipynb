{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "We first load and preprocess load demand data from [Albania](https://data.dtu.dk/articles/dataset/Albanian_national_electricity_consumption_and_weather_conditions_for_2016-2019/22786922) from a Parquet file format. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet(\"../data/albania_res.parquet\")\n",
    "data.index = pd.to_datetime(data.index, utc=\"UTC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we  sett up data transformation and scaling for time series forecasting. The goal is to prepare the dataset for modeling by defining the necessary parameters for the transformation and scaling processes.\n",
    "\n",
    "For this purpose we use ``DatasetObjective`` A utility from mlpforecast for managing dataset objectives, crucial for time series forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlpforecast.data.transform import DatasetObjective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To achive this we define set of variables as follow\n",
    "\n",
    "- target_series: The target variable to be predicted; in this case, \"NetLoad\".\n",
    "- unknown_features: Any fnumerical eatures not observed in the future etc demand price.\n",
    "- calendar_variables: Features derived from the calendar, such as hour and session.\n",
    "- known_calendar_features: Precomputed features that represent the calendar variables.\n",
    "- known_continuous_features: Continuous features used in the modeling process, including lagged variables and temperature.\n",
    "- input_window_size: The size of the input window for the model (96 time steps).\n",
    "- forecast_horizon: The forecasting horizon (48 time steps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_params = {\n",
    "    \"target_series\": [\"NetLoad\"],\n",
    "    \"unknown_features\": [],\n",
    "    \"calendar_variables\": [\"HOUR\", \"Session\"],\n",
    "    \"known_calendar_features\": [\"HOUR-cosin\", \"Session-cosin\"],\n",
    "    \"known_continuous_features\": [\"NetLoad_lag_48\", \"NetLoad_lag_336\", \"Temperature\"],\n",
    "    \"input_window_size\": 96,\n",
    "    \"forecast_horizon\": 48,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we specify how we want to transform the data adding more features such as lagging or rolling window and scaling function using sklaern processing scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import (\n",
    "    MinMaxScaler,\n",
    "    RobustScaler,\n",
    "    SplineTransformer,\n",
    "    PowerTransformer,\n",
    ")\n",
    "\n",
    "data_params = {\n",
    "    \"input_scaler\": PowerTransformer(),\n",
    "    \"target_scaler\": PowerTransformer(),\n",
    "    \"lags\": [1, 7],\n",
    "    \"windows\": [],\n",
    "    \"window_funcs\": [\"mean\"],\n",
    "    \"period\": \"30min\",\n",
    "    \"date_column\": \"timestamp\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We update the parameters and define the  ``DatasetObjective``. This line ```data_params.update(common_params)`` merges common_params into data_params, ensuring all relevant settings are included for the dataset transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_params.update(common_params)\n",
    "ds = DatasetObjective(**data_params)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Since the dataset objective is an instance of sklearn object it implement two main function ```fit``` and ```transform```. The ```fit```  fit the DatasetObjective object (ds) to the provided data, ensuring that the transformations and scaling parameters are applied correctly to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.fit(data.reset_index())\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This fitted data processing pipeline associated with the DatasetObjective instance contains all the transformation steps that will be applied to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.data_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply the fitted transform to the data we call ```transform``` function. The transformed data will produce sequences of features and targets that can be directly consumed by a model, streamlining the forecasting process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = ds.transform(data.reset_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fitting and transforming the dataset, you can proceed to analyze the transformed data, and train forecasting models using the prepared feature and target sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "az.style.use([\"science\", \"arviz-doc\", \"tableau-colorblind10\"])\n",
    "nice_fonts = {\n",
    "    \"savefig.bbox\": \"tight\",\n",
    "    \"savefig.pad_inches\": 0.05,\n",
    "    \"axes.labelsize\": 8,\n",
    "    \"font.size\": 8,\n",
    "    \"legend.fontsize\": 8,\n",
    "    \"legend.frameon\": False,\n",
    "}\n",
    "matplotlib.rcParams.update(nice_fonts)\n",
    "import matplotlib_inline.backend_inline\n",
    "\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats(\n",
    "    \"svg\", \"pdf\", \"retina\"\n",
    ")  # For export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Visualize the transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(4, 2))\n",
    "ax.plot(x[0, :, :4]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Forecasting Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build the forecasting model, we first create an instance of ```MLPForecast```, a forecasting model from mlpforecast designed to handle time series forecasting tasks. One of the inputs to ```MLPForecast``` is the data pipeline created in the preceding cell. The  we define the following `model_hparams` as a dictionary.\n",
    "\n",
    "- **data_pipeline**: The fitted data pipeline from the previous steps.\n",
    "- **embedding_size**: Size of the embedding layer.\n",
    "- **embedding_type**: Type of embedding to use (set to `None` here).\n",
    "- **combination_type**: Method for combining features (set to `\"addition-comb\"`).\n",
    "- **hidden_size**: Number of hidden units in the model (256).\n",
    "- **num_layers**: Number of layers in the model (2).\n",
    "- **expansion_factor**: Factor for expanding the dimensionality of the data (2).\n",
    "- **residual**: Whether to use residual connections (set to `True`).\n",
    "- **activation_function**: Activation function for the model (set to `\"SiLU\"`).\n",
    "- **out_activation_function**: Activation function for output (set to `\"Identity\"`).\n",
    "- **dropout_rate**: Rate of dropout for regularization (0.2).\n",
    "- **alpha**: Parameter for the model (0.5).\n",
    "- **max_epochs**: Maximum number of epochs for training (10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlpforecast.forecaster.mlp import MLPForecast\n",
    "\n",
    "model_hparams = {\n",
    "    \"data_pipeline\": ds,\n",
    "    \"embedding_size\": 20,\n",
    "    \"embedding_type\": None,\n",
    "    \"combination_type\": \"addition-comb\",\n",
    "    \"hidden_size\": 256,\n",
    "    \"num_layers\": 2,\n",
    "    \"expansion_factor\": 2,\n",
    "    \"residual\": True,\n",
    "    \"activation_function\": \"SiLU\",\n",
    "    \"out_activation_function\": \"Identity\",\n",
    "    \"dropout_rate\": 0.2,\n",
    "    \"alpha\": 0.5,\n",
    "    \"max_epochs\": 10,\n",
    "}\n",
    "model_hparams.update(common_params)\n",
    "model = MLPForecast(\n",
    "    exp_name=\"test\", model_type=\"MLPF\", hparams=model_hparams, rich_progress_bar=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "To train the model, use the following code:\n",
    "\n",
    "```python\n",
    "model.fit()\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(data.reset_index()[:20000])\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make predictions, use the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.predict(data.reset_index()[20000 : 20000 + 48 * 7 * 2])\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command predicts the next 48*7*2 time steps based on the dataset. we can visualize the predictions as follows using ```mlpforecast.plot``` functionality\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlpforecast.plot.visual_functions import plot_prediction\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 2))\n",
    "ax = plot_prediction(ax, true=out[\"NetLoad\"].values, mu=out[\"NetLoad_forecast\"].values)\n",
    "ax.set_ylim(0, 1500)\n",
    "ax.set_ylabel(\"Power (MWh)\")\n",
    "fig.tight_layout(pad=1.08, h_pad=0.5, w_pad=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code create a plot showing the true values versus the predicted values for \"NetLoad\", with the y-axis labeled in MWh and proper layout settings applied.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "met=['RMSE',  'MAE', 'MAPE', 'CORR', 'NBIAS', 'R2-error', 'SMAPE']\n",
    "model.metrics.groupby(\"target\")[met].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backtesting the Model\n",
    "The library allows you to perform backtesting to evaluate the model across multiple time splits. Use the following code to set up backtesting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlpforecast.evaluation.backtester import BacktestingForecast\n",
    "\n",
    "backtest = BacktestingForecast(\n",
    "    forecast_len=4,\n",
    "    incremental_len=2,\n",
    "    min_train_len=12,\n",
    "    n_splits=10,\n",
    "    window_type=\"expanding\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you need to set up the model using partial:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "model = partial(MLPForecast, hparams=model_hparams, exp_name=\"bactesting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform the backtesting, run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df, metric_df = backtest.fit(data.reset_index(), model_instance=model)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to evaluate the results, you can compute the mean of metrics across folds:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df.groupby([\"target\", \"Folds\"])[\"MSE\", \"SMAPE\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-13 16:03:58,467] Using an existing study with name 'Tanesco' instead of creating a new one.\n",
      "INFO:lightning_fabric.utilities.seed:Seed set to 42\n",
      "[W 2024-07-13 16:03:59,082] Trial 2 failed with parameters: {'embedding_size': 32, 'hidden_size': 276, 'num_layers': 5, 'expansion_factor': 4, 'embedding_type': 'PosEmb', 'combination_type': 'weighted-comb', 'residual': True, 'activation_function': 'Identity', 'out_activation_function': 'SELU', 'dropout_rate': 0.4, 'alpha': 0.2} because of the following error: ImportError(\"Tried to import 'lightning' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'lightning'.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/anthonyfaustine/opt/anaconda3/envs/MLPF/lib/python3.11/site-packages/optuna_integration/pytorch_lightning.py\", line 15, in <module>\n",
      "    import lightning.pytorch as pl\n",
      "ModuleNotFoundError: No module named 'lightning'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/anthonyfaustine/opt/anaconda3/envs/MLPF/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/Users/anthonyfaustine/Documents/projects/mlpforecast/mlpforecast/forecaster/mlp.py\", line 144, in objective\n",
      "    val_cost = model.fit(self.train_df, self.validation_df)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/anthonyfaustine/Documents/projects/mlpforecast/mlpforecast/forecaster/common.py\", line 228, in fit\n",
      "    self._set_up_trainer()\n",
      "  File \"/Users/anthonyfaustine/Documents/projects/mlpforecast/mlpforecast/forecaster/common.py\", line 122, in _set_up_trainer\n",
      "    early_stopping = PyTorchLightningPruningCallback(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/anthonyfaustine/opt/anaconda3/envs/MLPF/lib/python3.11/site-packages/optuna_integration/pytorch_lightning.py\", line 58, in __init__\n",
      "    _imports.check()\n",
      "  File \"/Users/anthonyfaustine/opt/anaconda3/envs/MLPF/lib/python3.11/site-packages/optuna/_imports.py\", line 89, in check\n",
      "    raise ImportError(message) from exc_value\n",
      "ImportError: Tried to import 'lightning' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'lightning'.\n",
      "[W 2024-07-13 16:03:59,084] Trial 2 failed with value None.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Tried to import 'lightning' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'lightning'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/MLPF/lib/python3.11/site-packages/optuna_integration/pytorch_lightning.py:15\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m optuna\u001b[38;5;241m.\u001b[39m_imports\u001b[38;5;241m.\u001b[39mtry_import() \u001b[38;5;28;01mas\u001b[39;00m _imports:\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpl\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LightningModule\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lightning'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m  MLPForecast(hparams\u001b[38;5;241m=\u001b[39mmodel_hparams, exp_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhyper\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m model\u001b[38;5;241m.\u001b[39mauto_tune(data\u001b[38;5;241m.\u001b[39mreset_index()[:\u001b[38;5;241m20000\u001b[39m], data\u001b[38;5;241m.\u001b[39mreset_index()[:\u001b[38;5;241m20000\u001b[39m])\n",
      "File \u001b[0;32m~/Documents/projects/mlpforecast/mlpforecast/forecaster/mlp.py:162\u001b[0m, in \u001b[0;36mMLPForecast.auto_tune\u001b[0;34m(self, train_df, val_df, num_trial, reduction_factor, patience)\u001b[0m\n\u001b[1;32m    152\u001b[0m pruner \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mpruners\u001b[38;5;241m.\u001b[39mPatientPruner(\n\u001b[1;32m    153\u001b[0m     base_pruner, patience\u001b[38;5;241m=\u001b[39mpatience, min_delta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    154\u001b[0m )\n\u001b[1;32m    155\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(\n\u001b[1;32m    156\u001b[0m     direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    157\u001b[0m     pruner\u001b[38;5;241m=\u001b[39mpruner,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m     load_if_exists\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    161\u001b[0m )\n\u001b[0;32m--> 162\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(\n\u001b[1;32m    163\u001b[0m     objective,\n\u001b[1;32m    164\u001b[0m     n_trials\u001b[38;5;241m=\u001b[39mnum_trial,  \u001b[38;5;66;03m# Default to 100 trials if not specified\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[print_callback],\n\u001b[1;32m    166\u001b[0m )\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhparams\u001b[38;5;241m.\u001b[39mupdate(study\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    168\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/best_params.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m, study\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mparams\n\u001b[1;32m    170\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/MLPF/lib/python3.11/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     _optimize(\n\u001b[1;32m    452\u001b[0m         study\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    453\u001b[0m         func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m    454\u001b[0m         n_trials\u001b[38;5;241m=\u001b[39mn_trials,\n\u001b[1;32m    455\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    456\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[1;32m    457\u001b[0m         catch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m(catch) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(catch, Iterable) \u001b[38;5;28;01melse\u001b[39;00m (catch,),\n\u001b[1;32m    458\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    459\u001b[0m         gc_after_trial\u001b[38;5;241m=\u001b[39mgc_after_trial,\n\u001b[1;32m    460\u001b[0m         show_progress_bar\u001b[38;5;241m=\u001b[39mshow_progress_bar,\n\u001b[1;32m    461\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/MLPF/lib/python3.11/site-packages/optuna/study/_optimize.py:62\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 62\u001b[0m         _optimize_sequential(\n\u001b[1;32m     63\u001b[0m             study,\n\u001b[1;32m     64\u001b[0m             func,\n\u001b[1;32m     65\u001b[0m             n_trials,\n\u001b[1;32m     66\u001b[0m             timeout,\n\u001b[1;32m     67\u001b[0m             catch,\n\u001b[1;32m     68\u001b[0m             callbacks,\n\u001b[1;32m     69\u001b[0m             gc_after_trial,\n\u001b[1;32m     70\u001b[0m             reseed_sampler_rng\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     71\u001b[0m             time_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     72\u001b[0m             progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar,\n\u001b[1;32m     73\u001b[0m         )\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/MLPF/lib/python3.11/site-packages/optuna/study/_optimize.py:159\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/MLPF/lib/python3.11/site-packages/optuna/study/_optimize.py:247\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    243\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    246\u001b[0m ):\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/MLPF/lib/python3.11/site-packages/optuna/study/_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m func(trial)\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "File \u001b[0;32m~/Documents/projects/mlpforecast/mlpforecast/forecaster/mlp.py:144\u001b[0m, in \u001b[0;36mMLPForecast.auto_tune.<locals>.objective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhparams\u001b[38;5;241m.\u001b[39mupdate(params)\n\u001b[1;32m    136\u001b[0m model \u001b[38;5;241m=\u001b[39m MLPForecast(\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhparams,\n\u001b[1;32m    138\u001b[0m     exp_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexp_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    142\u001b[0m     file_name\u001b[38;5;241m=\u001b[39mtrial\u001b[38;5;241m.\u001b[39mnumber\n\u001b[1;32m    143\u001b[0m )\n\u001b[0;32m--> 144\u001b[0m val_cost \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_df, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidation_df)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m val_cost\n",
      "File \u001b[0;32m~/Documents/projects/mlpforecast/mlpforecast/forecaster/common.py:228\u001b[0m, in \u001b[0;36mPytorchForecast.fit\u001b[0;34m(self, train_df, val_df, train_ratio, drop_last, num_worker, batch_size, pin_memory)\u001b[0m\n\u001b[1;32m    216\u001b[0m train_feature, train_target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdata_pipeline\u001b[38;5;241m.\u001b[39mtransform(train_df)\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatamodule \u001b[38;5;241m=\u001b[39m TimeseriesDataModule(\n\u001b[1;32m    218\u001b[0m     train_inputs\u001b[38;5;241m=\u001b[39mtrain_feature,\n\u001b[1;32m    219\u001b[0m     train_targets\u001b[38;5;241m=\u001b[39mtrain_target,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    225\u001b[0m     pin_memory\u001b[38;5;241m=\u001b[39mpin_memory,\n\u001b[1;32m    226\u001b[0m )\n\u001b[0;32m--> 228\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_up_trainer()\n\u001b[1;32m    229\u001b[0m start_time \u001b[38;5;241m=\u001b[39m default_timer()\n\u001b[1;32m    230\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m---------------Training started ---------------------------\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/projects/mlpforecast/mlpforecast/forecaster/common.py:122\u001b[0m, in \u001b[0;36mPytorchForecast._set_up_trainer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrial \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     early_stopping \u001b[38;5;241m=\u001b[39m PyTorchLightningPruningCallback(\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrial, monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric\n\u001b[1;32m    124\u001b[0m     )\n\u001b[1;32m    125\u001b[0m     callback\u001b[38;5;241m.\u001b[39mappend(early_stopping)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/MLPF/lib/python3.11/site-packages/optuna_integration/pytorch_lightning.py:58\u001b[0m, in \u001b[0;36mPyTorchLightningPruningCallback.__init__\u001b[0;34m(self, trial, monitor)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial: optuna\u001b[38;5;241m.\u001b[39mtrial\u001b[38;5;241m.\u001b[39mTrial, monitor: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     _imports\u001b[38;5;241m.\u001b[39mcheck()\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trial \u001b[38;5;241m=\u001b[39m trial\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/MLPF/lib/python3.11/site-packages/optuna/_imports.py:89\u001b[0m, in \u001b[0;36m_DeferredImportExceptionContextManager.check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deferred \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     88\u001b[0m     exc_value, message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deferred\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(message) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc_value\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: Tried to import 'lightning' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'lightning'."
     ]
    }
   ],
   "source": [
    "model =  MLPForecast(hparams=model_hparams, exp_name=\"hyper\")\n",
    "model.auto_tune(data.reset_index()[:20000], data.reset_index()[:20000])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLPF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
