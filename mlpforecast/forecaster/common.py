from __future__ import annotations
import glob
from pathlib import Path
from timeit import default_timer

import pytorch_lightning as pl
from optuna_integration.pytorch_lightning import PyTorchLightningPruningCallback
from pytorch_lightning.callbacks import (
    EarlyStopping,
    LearningRateMonitor,
    ModelCheckpoint,
    RichProgressBar,
    TQDMProgressBar,
)
from pytorch_lightning.callbacks.progress.rich_progress import RichProgressBarTheme
from pytorch_lightning import loggers




def get_latest_checkpoint(checkpoint_path):
    checkpoint_path = str(checkpoint_path)
    list_of_files = glob.glob(checkpoint_path + "/*.ckpt")

    if list_of_files:
        latest_file = max(list_of_files, key=os.path.getctime)
    else:
        latest_file = None
    return latest_file


class PytorchForecast:
    """
    PytorchForecast class for setting up and managing the training process of a PyTorch model using PyTorch Lightning.

    Attributes:
        exp_name (str): The name of the experiment.
        file_name (str): The name of the file to save the logs and model checkpoints.
        seed (int): The seed for random number generation to ensure reproducibility.
        root_dir (str): The root directory for saving logs and checkpoints.
        trial (optuna.trial.Trial): Optuna trial for hyperparameter optimization.
        metric (str): The metric to monitor for early stopping and model checkpointing.
        max_epochs (int): The maximum number of training epochs.
        wandb (bool): Flag to use Wandb logger instead of TensorBoard logger.
        rich_progress_bar (bool): Flag to use rich progress bar for training visualization.
    """

    def __init__(
        self,
        exp_name="Tanesco",
        file_name=None,
        seed=42,
        root_dir="../",
        trial=None,
        metric="val_mae",
        max_epochs=10,
        wandb=False,
        model_type="MLPF",
        rich_progress_bar=False,
        gradient_clip_val=None
    ):
        """
        Initializes the PytorchForecast class with the given parameters.

        Parameters:
            exp_name (str): The name of the experiment. Defaults to "Tanesco".
            file_name (str): The name of the file to save the logs and model checkpoints. Defaults to None.
            seed (int): The seed for random number generation. Defaults to 42.
            root_dir (str): The root directory for saving logs and checkpoints. Defaults to "../".
            trial (optuna.trial.Trial): Optuna trial for hyperparameter optimization. Defaults to None.
            metric (str): The metric to monitor for early stopping and model checkpointing. Defaults to "val_mae".
            max_epochs (int): The maximum number of training epochs. Defaults to 10.
            wandb (bool): Flag to use Wandb logger instead of TensorBoard logger. Defaults to False.
            rich_progress_bar (bool): Flag to use rich progress bar for training visualization. Defaults to True.
        """
        super().__init__()

        self.exp_name = exp_name
        self.file_name = file_name
        self.seed = seed
        self.root_dir = root_dir
        self.trial = trial
        self.metric = metric
        self.max_epochs = max_epochs
        self.wandb = wandb
        self.model_type = model_type
        self.rich_progress_bar = rich_progress_bar
        self.model_type = "default_model"  # Update with actual model type
        self.model = None
        self.gradient_clip_val=gradient_clip_val
        self._create_folder()

    def _create_folder(self):
        """
        Creates directories for storing the results, logs, and figures generated by the model.
        """
        self.results_path = Path(
            f"{self.root_dir}/results/{self.exp_name}/{self.model_type}/"
        )

        self.logs = Path(f"{self.root_dir}/logs/{self.exp_name}/{self.model_type}/")
        self.figures = Path(
            f"{self.root_dir}/figures/{self.exp_name}/{self.model_type}/"
        )
        self.figures.mkdir(parents=True, exist_ok=True)
        self.logs.mkdir(parents=True, exist_ok=True)
        self.results_path.mkdir(parents=True, exist_ok=True)
        if self.file_name is not None:
            self.checkpoints = Path(
                f"{self.root_dir}/checkpoints/{self.exp_name}/{self.model_type}/{self.file_name}"
            )
        else:
            self.checkpoints = Path(
                f"{self.root_dir}/checkpoints/{self.exp_name}/{self.model_type}"
            )
        self.checkpoints.mkdir(parents=True, exist_ok=True)

    def _set_up_trainer(self):
        callback = []
        pl.seed_everything(self.seed, workers=True)
        if self.trial is not None:
            self.logger = (
                True  # DictLogger(self.logs,  version=self.trial.number)
            )
            early_stopping = PyTorchLightningPruningCallback(
                self.trial, monitor=self.metric
            )
            callback.append(early_stopping)
        else:
            early_stopping = EarlyStopping(
                monitor=self.metric,
                min_delta=0.0,
                patience=int(self.hparams["max_epochs"] * 0.5),
                verbose=False,
                mode="min",
                check_on_train_epoch_end=True,
            )
            callback.append(early_stopping)
            if not self.wandb:

                self.logger = loggers.TensorBoardLogger(
                    save_dir=self.logs,
                    version=(
                        self.file_name if self.file_name is not None else 0
                    ),
                )
            else:
                self.logger = loggers.WandbLogger(
                    save_dir=self.logs,
                    name=self.file_name,
                    project=self.exp_name,
                    log_model="all",
                )
                # self.logger.watch(self.model)
                # log gradients and model topology

            self.checkpoints.mkdir(parents=True, exist_ok=True)
            checkpoint_callback = ModelCheckpoint(
                dirpath=self.checkpoints,
                monitor=self.metric,
                mode="min",
                save_top_k=1,
                filename="{epoch:02d}",
            )
            callback.append(checkpoint_callback)
            lr_logger = LearningRateMonitor()
            callback.append(lr_logger)

        if self.rich_progress_bar:
            progress_bar = RichProgressBar(
                theme=RichProgressBarTheme(
                    description="green_yellow",
                    progress_bar="green1",
                    progress_bar_finished="green1",
                    progress_bar_pulse="#6206E0",
                    batch_progress="green_yellow",
                    time="grey82",
                    processing_speed="grey82",
                    metrics="grey82",
                )
            )
        else:
            progress_bar = TQDMProgressBar()

        callback.append(progress_bar)

        self.trainer = pl.Trainer(
            logger=self.logger,
            gradient_clip_val=self.clipping_value,
            max_epochs=self.max_epochs,
            callbacks=callback,
            accelerator="auto",
            devices=1,
        )